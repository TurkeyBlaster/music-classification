{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkeyBlaster/music-classification/blob/master/Music_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TbVmkFvwcIM",
        "colab_type": "text"
      },
      "source": [
        "# Music Classification\n",
        "Ananth Madan\n",
        "\n",
        "This was an idea I got from reading <a href=https://towardsdatascience.com/using-cnns-and-rnns-for-music-genre-recognition-2435fb2ed6af>this article</a> and thought would be an interesting step-up in projects. Prior to this, I had only done tutorial projects and copied others in an attempt to understand the syntax and process of Deep Learning through Keras and Sklearn. *This* project was my first experience of writing code on my own (aided by StackOverflow and the Keras API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6mlDUUB1Pv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAChN6w0LNuW",
        "colab_type": "code",
        "outputId": "4a0bbd22-c33d-4a44-c5c6-9403b9f1613b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "drive.mount('/content/gdrive') # Mounting drive to colab to access data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRYHvGMG6lDj",
        "colab_type": "code",
        "outputId": "bb0e9f3f-b5fe-450d-8133-37ee1cb9fd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.1.0 # Installing tensorflow 2.0 (as per the tensorflow API)\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  import tensorflow.compat.v2 as tf\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 31kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.17.5)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.1.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.16.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atrAYer6Mkkq",
        "colab_type": "code",
        "outputId": "9224ffc0-0391-447e-c841-1f4f2482b0b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!pip install scikit-optimize # Installing scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/ff/4cd204e8ad092d7db2ddf383adc3747166117915a6a47df025c6b727a500/scikit_optimize-0.7.1-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.14.1)\n",
            "Collecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/35/1e/eda9fe07f752ced7afcef590e7d74390f0d9c9c0b7ff98317afbaa0697e3/pyaml-19.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.17.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-19.12.0 scikit-optimize-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S_MUT0B8OAu",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "Each of the following imports will help in one of four categories:\n",
        "* Data Preprocessing: Importing and transforming data\n",
        "* Model Construction: Building the Neural Network\n",
        "* Model Optimization: Tuning the hyperparameters of the model\n",
        "* Model Visualization: Displaying the results of the model and history\n",
        "\n",
        "Each of these categories is vital and are made significantly easier with the following imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJnVJ8_U6Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Flatten, MaxPool2D\n",
        "from tensorflow.keras.layers import GRU, Bidirectional, Embedding, Lambda, LayerNormalization\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "from skopt import gbrt_minimize, gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2lXkZ1_93du",
        "colab_type": "text"
      },
      "source": [
        "# Importing Data\n",
        "After uploading the data to our drive and mounting it to colab, we can import it. In this case, the data has been compressed as numpy <a href=https://imageio.readthedocs.io/en/stable/format_npz.html>npz</a> files, where each file contain two arrays: the first contains the arrays that describe the spectrograms of each song, and the second contains the genres, encoded as a digit from (1-7). Using numpy, we can easily load and extract the data from the npz files. Furthermore, we can plot the data using librosa and matplotlib to refamiliarize ourselves with what we are working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDTlt147Lg3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/WWHS AI/Data' # Getting the folder path to the data folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3NUVElfUcgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using numpy to load the specified file from the root folder\n",
        "# as 'train_npz'\n",
        "with np.load(root_path + '/train_shuffled.npz') as train_npz:\n",
        "\n",
        "  X_train = train_npz['arr_0'] # The first array becomes the training data\n",
        "  y_train = train_npz['arr_1'] # The second array becomes the training labels\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZKowzjz4V54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same process as the training data\n",
        "with np.load(root_path + '/valid_shuffled.npz') as valid_npz:\n",
        "\n",
        "  X_valid = valid_npz['arr_0']\n",
        "  y_valid = valid_npz['arr_1']\n",
        "\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRuGCFcE2sTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same process as the training data\n",
        "with np.load(root_path + '/test.npz') as test_npz:\n",
        "\n",
        "  X_test = test_npz['arr_0']\n",
        "  y_test = test_npz['arr_1']\n",
        "\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4h4gjk8yGsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary mapping number to actual genre\n",
        "reverse_map = {\n",
        "    \n",
        "    0: 'Electronic',\n",
        "    1: 'Experimental',\n",
        "    2: 'Folk',\n",
        "    3: 'Hip-Hop',\n",
        "    4: 'Instrumental',\n",
        "    5: 'International',\n",
        "    6: 'Pop',\n",
        "    7: 'Rock'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMxBVByCyeoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To test if our data is still functional, we can replot it\n",
        "def plot_rand_mels(mels, genres, reverse_map=reverse_map):\n",
        "\n",
        "  # Matplotlib figure customization\n",
        "  plt.figure(figsize=(20, 8))\n",
        "  plt.subplots_adjust(wspace=0.2, hspace=0.35)\n",
        "\n",
        "  ind = np.random.randint(mels.shape[0]) # Start at a random part in the array\n",
        "  seeking_genre = 0 # The current genre that is being sought\n",
        "\n",
        "  # Iterating through the genres\n",
        "  while seeking_genre < 8:\n",
        "\n",
        "    if genres[ind] == seeking_genre:\n",
        "\n",
        "      # Plot the mel-Spectrogram in a respective subplot\n",
        "      plt.subplot(2, 4, seeking_genre + 1)\n",
        "\n",
        "      librosa.display.specshow(mels[ind], x_axis='time')\n",
        "      plt.colorbar(format='%+2.0f dB')\n",
        "      plt.title(reverse_map[genres[ind]]) # Converting the numbers to their respective genres\n",
        "\n",
        "      seeking_genre += 1 # Change the genre sought after\n",
        "\n",
        "    ind += 1\n",
        "\n",
        "    # Loop to the beginning if we reach the end\n",
        "    if ind == mels.shape[0]:\n",
        "\n",
        "      ind = 0\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joDwdNRbDBiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_rand_mels(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tSu76kSE7Zw",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "The main focus of this project is the actual model, and for the most part, the two categories are Construction and Optimization. In this case, the framework of the model built and then fit to be retroactively tuned by a Bayesian algorithm.\n",
        "\n",
        "## Construction\n",
        "This model implements a structure dictated in <a href=https://arxiv.org/abs/1712.08370>this paper</a>.\n",
        "\n",
        "## Optimization\n",
        "The model implements Bayesian Tuning (via skopt), with the surrogate functions being a standard Gaussian Process and a Gradient Boosted Regressor Tree algorithm (separately)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RKIqLne8avf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "num_classes = 8\n",
        "mel_features = X_train.shape[1]\n",
        "mel_time = X_train.shape[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVsdgkOuOVxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dims = [\n",
        "        \n",
        "        Real(low=1e-4, high=1e1, prior='log-uniform', name='learning_rate'),\n",
        "        Categorical([Adam, RMSprop, Nadam], name='optimizer'),\n",
        "\n",
        "        Integer(low=1, high=128, name='batch_size'),\n",
        "        Integer(10, 60, name='epochs'),\n",
        "\n",
        "        Integer(5, 8, name='num_conv_layers'),\n",
        "        Integer(16, 256, name='num_conv_filters_1'),\n",
        "        Integer(16, 256, name='num_conv_filters_2'),\n",
        "        Integer(16, 256, name='num_conv_filters_3'),\n",
        "        Integer(16, 256, name='num_conv_filters_4'),\n",
        "        Integer(16, 256, name='num_conv_filters_5'),\n",
        "        Integer(16, 256, name='num_conv_filters_6'),\n",
        "        Integer(16, 256, name='num_conv_filters_7'),\n",
        "        Integer(16, 256, name='num_conv_filters_8'),\n",
        "\n",
        "        Categorical(categories=['relu', 'elu', 'selu'], name='conv_activ_1'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_2'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_3'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_4'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_5'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_6'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_7'),\n",
        "        Categorical(['relu', 'elu', 'selu'], name='conv_activ_8'),\n",
        "\n",
        "        Integer(16, 256, name='num_gru_units'),\n",
        "        Real(0.0, 0.5, name='gru_dropout_rate')\n",
        "]\n",
        "\n",
        "default_dims = [\n",
        "                \n",
        "                1e-3,\n",
        "                Adam,\n",
        "\n",
        "                32,\n",
        "                15,\n",
        "\n",
        "                5,\n",
        "                16,\n",
        "                32,\n",
        "                64,\n",
        "                64,\n",
        "                64,\n",
        "                128,\n",
        "                128,\n",
        "                256,\n",
        "\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "                'relu',\n",
        "\n",
        "                64,\n",
        "                0.0\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19jdzqWRBlIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actual Model\n",
        "def parallel_model(model_input, **params):\n",
        "  \n",
        "  # CNN Layer\n",
        "  conv_input = model_input\n",
        "\n",
        "  # Pool Sizes for each Variable Size of 5-8\n",
        "  pool_sizes_dict = {\n",
        "      \n",
        "      5: [(2, 2), (2, 2), (2, 2), (4, 4), (4, 4)],\n",
        "      6: [(2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (4, 4)],\n",
        "      7: [(2, 2), (2, 2), None, (2, 2), (2, 2), (2, 2), (4, 4)],\n",
        "      8: [(2, 2), (2, 2), None, (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)],\n",
        "  }\n",
        "\n",
        "  num_conv_layers = params['num_conv_layers']\n",
        "  pool_sizes = pool_sizes_dict[num_conv_layers]\n",
        "  for i in range(num_conv_layers):\n",
        "    \n",
        "    # Conv2D Layer\n",
        "    conv_input = Conv2D(\n",
        "        filters = params[f'num_conv_filters_{i + 1}'],\n",
        "        kernel_size = (1, 3),\n",
        "        activation = params[f'conv_activ_{i + 1}'],\n",
        "        name = f'Conv{i + 1}'\n",
        "        ) (conv_input)\n",
        "\n",
        "    # Pooling Layer\n",
        "    pool_size = pool_sizes[i]\n",
        "    if pool_size != None:\n",
        "      conv_input = MaxPool2D(pool_size, name=f'Pool{i + 1}') (conv_input)\n",
        "\n",
        "    # Normalizing Layer\n",
        "    if i != num_conv_layers - 1:\n",
        "        conv_input = BatchNormalization(name=f'Norm{i + 1}') (conv_input)\n",
        "\n",
        "  flat = Flatten(name='flat') (conv)\n",
        "\n",
        "  # RNN Layer\n",
        "  pool_rnn = MaxPool2D((2, 4), name='PoolRNN') (model_input)\n",
        "  squeeze = Lambda(lambda x: K.squeeze(x, axis=-1), name='Squeeze') (pool_rnn)\n",
        "  bigru = Bidirectional(GRU(\n",
        "      units = params['num_gru_units'],\n",
        "      recurrent_dropout = params['gru_dropout_rate'],\n",
        "      name = 'BiGRU'\n",
        "      )) (squeeze)\n",
        "\n",
        "  concat = concatenate([flat, bigru], name='Concat') # Merge layers\n",
        "  model_output = Dense(num_classes, activation='sigmoid', name='Output') (concat) # Sigmoid\n",
        "\n",
        "  model = Model(model_input, model_output, name='Parallel_Model')\n",
        "  \n",
        "  # Compile Parameters\n",
        "  model.compile(\n",
        "      loss = 'categorical_crossentropy',\n",
        "      optimizer = params['optimizer'] (lr=params['learning_rate']),\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPuy8DNUUP1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data generator to not crash memory\n",
        "def data_generator(data, labels, batch_size):\n",
        "\n",
        "  while True:\n",
        "\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "\n",
        "      yield data[i: i + batch_size], labels[i: i + batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30J37Y9Ax2J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to train model\n",
        "def train_model(train_data, train_labels, valid_data, valid_labels, model=None, **params):\n",
        "\n",
        "  batch_size = params['batch_size']\n",
        "\n",
        "  train_data = K.expand_dims(train_data, axis=-1)\n",
        "  valid_data = K.expand_dims(valid_data, -1)\n",
        "\n",
        "  train_generator = data_generator(train_data, train_labels, batch_size=batch_size)\n",
        "  valid_generator = data_generator(valid_data, valid_labels, batch_size)\n",
        "\n",
        "  reduce_lr = ReduceLROnPlateau(\n",
        "      \n",
        "      monitor = 'val_accuracy',\n",
        "      factor = 0.5,\n",
        "      min_delta = 0.01,\n",
        "      patience = 4,\n",
        "      mode = 'auto',\n",
        "      verbose = 1\n",
        "  )\n",
        "  \n",
        "  early_stop = EarlyStopping(\n",
        "      \n",
        "      monitor = 'val_accuracy',\n",
        "      min_delta = 0.001,\n",
        "      patience = 5,\n",
        "      mode = 'auto',\n",
        "      verbose = 1\n",
        "  )\n",
        "\n",
        "  if model == None:\n",
        "\n",
        "    model_input = Input(shape=(mel_features, mel_time, 1), name='Input')\n",
        "    model = parallel_model(model_input, **params)\n",
        "    print(model.summary())\n",
        "\n",
        "  history = model.fit(\n",
        "      \n",
        "      x = train_generator,\n",
        "      epochs = params['epochs'],\n",
        "      steps_per_epoch = train_data.shape[0] // batch_size,\n",
        "      validation_data = valid_generator,\n",
        "      validation_steps = valid_data.shape[0] // batch_size,\n",
        "      callbacks = [None],\n",
        "      verbose = 1\n",
        "  )\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwh81OoKX9fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to train model over several slices of data.\n",
        "# Regular tensor overflows the hardcoded limit (2GB)\n",
        "def train_in_slices(train_data, train_labels, valid_data, valid_labels, slices, **params):\n",
        "\n",
        "  model, history = None, None\n",
        "  \n",
        "  train_slice = train_data.shape[0] // slices\n",
        "  valid_slice = valid_data.shape[0] // slices\n",
        "\n",
        "  train_labels = to_categorical(train_labels)\n",
        "  valid_labels = to_categorical(valid_labels)\n",
        "\n",
        "  for i in range(slices):\n",
        "    \n",
        "    train_ind = train_slice * i\n",
        "    valid_ind = valid_slice * i\n",
        "\n",
        "    model, history = train_model(\n",
        "        \n",
        "        train_data = train_data[train_ind: train_ind + train_slice],\n",
        "        train_labels = train_labels[train_ind: train_ind + train_slice],\n",
        "        valid_data = valid_data[valid_ind: valid_ind + valid_slice],\n",
        "        valid_labels = valid_labels[valid_ind: valid_ind + valid_slice],\n",
        "        model = model,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6l3Y7NTEOaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model_hist = None\n",
        "best_preds = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56zPcU5Txf1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforming X_test for compatability\n",
        "X_test_expanded = np.expand_dims(X_test, -1)\n",
        "\n",
        "# Skopt Bayesian Optimization\n",
        "@use_named_args(dims)\n",
        "def fitness(**params):\n",
        "  \n",
        "  for key, val in params.items():\n",
        "\n",
        "    print(f'{key}: {val}')\n",
        "\n",
        "  blackbox, history = train_in_slices(X_train, y_train, X_valid, y_valid, 2, **params)\n",
        "\n",
        "  y_pred = blackbox.predict(X_test_expanded)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print(f'Model Accuracy: {acc:.02%}')\n",
        "\n",
        "  # Updating variables for later show\n",
        "  if best_acc is None or best_acc < acc:\n",
        "      \n",
        "      best_model_hist = hist\n",
        "      best_preds = y_pred\n",
        "\n",
        "  del blackbox\n",
        "  del history\n",
        "\n",
        "  K.clear_session()\n",
        "  tf.compat.v1.reset_default_graph()\n",
        "\n",
        "  return -acc # Returns negative as optimization tries to find minimum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lletMjKn398X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Gaussian Process as Surrogate\n",
        "gp_result = gp_minimize(\n",
        "    \n",
        "    func = fitness,\n",
        "    dimensions = dims,\n",
        "    n_calls = 25,\n",
        "    n_jobs = -1,\n",
        "    x0 = default_dims,\n",
        "    kappa = 5,\n",
        "    verbose = True\n",
        ")\n",
        "K.clear_session()\n",
        "tf.compat.v1.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELiurnu3CRoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(gp_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNCcxAac2Jg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GbujKI-gBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(best_model_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYBe6sZhFyQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classification_report(y_test, best_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVuh2s_P51sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_mat = confusion_matrix(y_test, best_preds)\n",
        "\n",
        "font_axis = {\n",
        "    'family': 'sans serif',\n",
        "    'color':  'darkred',\n",
        "    'weight': 'normal',\n",
        "    'size': 12,\n",
        "}\n",
        "\n",
        "font_title = {\n",
        "    'family': 'sans serif',\n",
        "    'color': 'black',\n",
        "    'weight': 'bold',\n",
        "    'size': 16\n",
        "}\n",
        "\n",
        "sns.heatmap(\n",
        "    data = conf_mat,\n",
        "    annot = True,\n",
        "    fmt = '',\n",
        "    cbar = False,\n",
        "    square = True,\n",
        "    xticklabels = reverse_map.values(),\n",
        "    yticklabels = reverse_map.values()\n",
        ")\n",
        "plt.title('HeatMap', fontdict=font_title, pad=10)\n",
        "plt.xlabel('Predicted', fontdict=font_axis, labelpad=10)\n",
        "plt.ylabel('Actual', fontdict=font_axis, labelpad=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}